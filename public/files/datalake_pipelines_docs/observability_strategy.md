# Observability Strategy: Current State vs. Gold Standard

## Executive Summary

This document outlines the comprehensive observability and quality gates strategy for the e-commerce data pipeline, addressing the question: **"What should we track, where should we store it, and what does production-grade look like?"**

---

## Storage Strategy: Local vs. Production

### Environment-Aware Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOCAL DEVELOPMENT (PIPELINE_ENV=local)                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Metrics: ./data/metrics/                                    â”‚
â”‚  Logs:    ./data/logs/                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Benefits:                                                    â”‚
â”‚  âœ… Zero configuration                                       â”‚
â”‚  âœ… Fast iteration                                           â”‚
â”‚  âœ… Easy debugging (grep/jq local files)                     â”‚
â”‚  âœ… Git-ignored (won't pollute repo)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEV TESTING (PIPELINE_ENV=dev)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Metrics: gs://<metrics-bucket>/pipeline_metrics/            â”‚
â”‚  Logs:    gs://<logs-bucket>/pipeline_logs/                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Benefits:                                                    â”‚
â”‚  âœ… Cloud-path parity with prod                              â”‚
â”‚  âœ… No local disk dependency                                 â”‚
â”‚  âœ… Suitable for CI or server runs                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCTION (PIPELINE_ENV=prod)                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Metrics: gs://ecom-datalake-metrics/pipeline_metrics/       â”‚
â”‚  Logs:    gs://ecom-datalake-logs/pipeline_logs/             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Benefits:                                                    â”‚
â”‚  âœ… Persisted with data (same lifecycle)                     â”‚
â”‚  âœ… Queryable in BigQuery (LOAD DATA FROM GCS)               â”‚
â”‚  âœ… Accessible to BI tools                                   â”‚
â”‚  âœ… Retained for compliance/auditing                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Principle

> **Metrics live alongside the data they describe.**

In production, storing metrics in the same GCS environment as your data means:
- Unified access controls (IAM)
- Same retention policies
- Easy correlation (metrics + data in same project)
- Queryable via BigQuery external tables

---

## What We Track

### 1. Pipeline Execution Metrics

**What:** End-to-end pipeline run metadata
**When:** Start of pipeline, end of pipeline, per-phase
**Why:** Performance monitoring, cost estimation, SLA tracking
**Location:** `data/metrics/pipeline_runs/`

**Key Metrics:**
- Run duration (total + per-phase)
- Status (SUCCESS/FAILED)
- Resource usage (memory, CPU, disk I/O)
- Parallel task execution times
- Trigger source (scheduler, manual, retry)

**Use Cases:**
- Detect performance regressions
- Optimize parallel execution
- Budget forecasting
- SLA compliance reporting

---

### 2. Bronze Layer Validation (Pre-Processing)

**What:** Metadata and sample validation before processing
**When:** DAG Phase 0 (before Silver transformation)
**Why:** Catch upstream issues early, validate assumptions
**Location:** `data/metrics/data_quality/`

**Key Metrics:**

#### 2a. Metadata Validation (Fast, Full Coverage)
- Row counts vs. expected (from SLA doc)
- Partition completeness (daily partitions present)
- Schema validation (columns match data contract)
- Freshness checks (hours since latest partition)
- File health (small files, size distribution)

#### 2b. Pydantic Sample Validation (Deeper, Sampled)
- Fail rate per table (sample of 1000-10000 rows)
- Error distribution (what validation rules failed)
- Anomaly detection (vs. historical baseline)
- Sample error records (for debugging)

**Use Cases:**
- Block processing if Bronze is incomplete
- Alert on schema drift
- Track upstream data quality trends
- Identify new error patterns

---

### 3. Silver Layer Quality (Post-Processing)

**What:** Transformation quality after dbt processing
**When:** After Base Silver completes (new DAG Phase 1.5)
**Why:** Validate transformation logic, SLA compliance
**Location:** `data/metrics/silver_quality/`

**Key Metrics:**
- Row counts: Bronze input vs. Silver output vs. Quarantine
- Pass rate per table (Silver / (Silver + Quarantine))
- Quarantine reason breakdown (what failed, how often)
- FK validation results (which relationships failed)
- Deduplication statistics
- Row loss (Bronze - (Silver + Quarantine))

**Use Cases:**
- Fail pipeline if pass rate < SLA threshold
- Alert on unexpected quarantine spikes
- Debug transformation logic issues
- Track quarantine trends over time

---

### 4. Enriched Silver (Polars Transformations)

**What:** Complex transformation performance and results
**When:** After each Polars runner completes
**Why:** Monitor business logic, optimize performance
**Location:** `data/metrics/enriched_silver/`

**Key Metrics:**
- Input row counts (all source tables)
- Output row counts
- Join match rates (e.g., attribution rate)
- Execution time (read, transform, write)
- Memory usage (peak, average)
- Output file statistics

**Use Cases:**
- Detect attribution rate changes
- Optimize Polars performance
- Track business metric trends
- Monitor memory consumption

---

### 5. Error & Exception Tracking

**What:** Structured error logs with full context
**When:** Any exception or validation failure
**Why:** Debugging, alerting, root cause analysis
**Location:** `data/logs/errors/` (JSONL format)

**Key Fields:**
- Timestamp, level, component
- Error type, message
- Full traceback
- Contextual data (table, row_count, etc.)
- Environment, run_id

**Use Cases:**
- Alert on critical errors (Slack, PagerDuty)
- Debug production issues
- Track error frequency/patterns
- Generate incident reports

**Production Note:**
- Object storage does not support efficient append. In dev/prod, logs are written as
  small, unique JSONL objects per event to avoid append bottlenecks.
- In a real deployment, prefer stdout logging and let the orchestrator ship logs
  to Cloud Logging/Datadog instead of writing directly to GCS.

---

### 6. Audit Trail

**What:** Record of all metric emissions
**When:** Any time a metric is logged
**Why:** Compliance, debugging, observability
**Location:** `data/logs/audit/` (JSONL format)

**Key Fields:**
- Metric name, value
- Tags/dimensions
- Timestamp, component
- Run context

**Use Cases:**
- Prove SLA compliance (auditable record)
- Track metric evolution over time
- Debug metric discrepancies
- Generate compliance reports

**Production Note:**
- Metrics are stored as JSON artifacts for portability and traceability.
- In production, also emit key metrics to a time-series backend (Prometheus/StatsD/Cloud Monitoring)
  for real-time alerting.

---

## Implementation Status

### âœ… Completed (Just Built)

1. **Observability Framework** (`src/observability/`)
   - Environment-aware config
   - Metrics writers (JSON files)
   - Structured logging (JSONL)
   - Local directory initialization

2. **Directory Structure**
   ```
   data/
   â”œâ”€â”€ metrics/
   â”‚   â”œâ”€â”€ pipeline_runs/
   â”‚   â”œâ”€â”€ data_quality/
   â”‚   â”œâ”€â”€ silver_quality/
   â”‚   â””â”€â”€ enriched_silver/
   â””â”€â”€ logs/
       â”œâ”€â”€ errors/
       â”œâ”€â”€ audit/
       â””â”€â”€ debug/
   ```

3. **Comprehensive Documentation**
   - `src/observability/README.md` - Usage guide
   - Schema reference
   - Integration examples

### â­ï¸ Next Steps (To Wire Into Pipeline)

1. **Expand Pydantic Schemas** (30 min)
   - Add missing table schemas
   - Update validation script

2. **Add Enriched Silver Metrics** (1 hour)
   - Emit join match rates, output row counts
   - Write to `data/metrics/enriched_silver/`

3. **Harden Silver Quality Gate (Prod Only)** (1 hour)
   - Fail on SLA breaches and FK mismatches in prod
   - Add semantic checks (cardinality/ratio)

---

## Production Deployment Checklist

When deploying to production:

### Infrastructure Setup

```bash
# 1. Create GCS buckets
gsutil mb -p my-project -l us-central1 gs://ecom-datalake-metrics
gsutil mb -p my-project -l us-central1 gs://ecom-datalake-logs

# 2. Set retention policies (e.g., 90 days for metrics, 30 for logs)
gsutil lifecycle set metrics-lifecycle.json gs://ecom-datalake-metrics

# 3. Grant Airflow service account access
gsutil iam ch serviceAccount:airflow@my-project.iam.gserviceaccount.com:roles/storage.objectAdmin \
  gs://ecom-datalake-metrics

gsutil iam ch serviceAccount:airflow@my-project.iam.gserviceaccount.com:roles/storage.objectAdmin \
  gs://ecom-datalake-logs

gsutil iam ch serviceAccount:airflow@my-project.iam.gserviceaccount.com:roles/storage.objectAdmin \
  gs://ecom-datalake-reports
```

### GCP Authentication

**Local Development** (uses Application Default Credentials):
```bash
# Authenticate with your personal account
gcloud auth application-default login

# docker.env (default)
USE_SA_AUTH=false
CLOUDSDK_CONFIG=/home/airflow/.config/gcloud
```

**Production** (uses Service Account):
```bash
# docker.env or Cloud Composer env vars
USE_SA_AUTH=true
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/.gcp/sa.json
```

See [CREDENTIALS_SAFETY_AUDIT.md](CREDENTIALS_SAFETY_AUDIT.md) for full GCP authentication details.

### Environment Configuration

```bash
# Add to Airflow environment variables (or docker.env)
PIPELINE_ENV=prod
OBSERVABILITY_ENV=prod
METRICS_BUCKET=ecom-datalake-metrics
LOGS_BUCKET=ecom-datalake-logs
REPORTS_BUCKET=ecom-datalake-reports

# Optional: Override to force local metrics/logs for testing
# OBSERVABILITY_ENV=local  # Forces local paths even when PIPELINE_ENV=prod
```

**Note**: See [CONFIG_STRATEGY.md](CONFIG_STRATEGY.md) for full configuration hierarchy and [ENVIRONMENT_VARIABLE_STRATEGY.md](ENVIRONMENT_VARIABLE_STRATEGY.md) for all observability env vars.

### BigQuery Setup (For Querying)

```sql
-- Create external table over metrics
CREATE EXTERNAL TABLE `my-project.observability.pipeline_runs`
OPTIONS (
  format = 'JSON',
  uris = ['gs://ecom-datalake-metrics/pipeline_metrics/pipeline_runs/*.json']
);

-- Query example: Daily pipeline success rate
SELECT
  DATE(metadata.written_at) as date,
  COUNTIF(run_metadata.status = 'SUCCESS') as successful_runs,
  COUNT(*) as total_runs,
  SAFE_DIVIDE(COUNTIF(run_metadata.status = 'SUCCESS'), COUNT(*)) as success_rate
FROM `my-project.observability.pipeline_runs`
GROUP BY date
ORDER BY date DESC
LIMIT 30;
```

---

## Recommended Tracking: Beyond the Basics

### Additional Metrics to Consider

1. **Cost Tracking**
   ```python
   log_metric("processing_cost_usd", 12.34, phase="silver", compute_type="duckdb")
   log_metric("storage_cost_usd", 5.67, layer="silver", table="orders")
   ```

2. **Data Lineage** (with Docker image versioning)
   ```python
   import os

   {
     "source_tables": ["bronze.orders", "bronze.customers"],
     "output_table": "silver.orders",
     "git_commit": os.getenv("GIT_COMMIT", "unknown"),
     "git_branch": os.getenv("GIT_BRANCH", "unknown"),
     "pipeline_version": os.getenv("PIPELINE_VERSION", "unknown"),
     "dbt_model_version": "1.0.5"
   }
   ```

   **Note**: Docker images are versioned with Git metadata (see [DOCKER_VERSIONING.md](DOCKER_VERSIONING.md)). The `GIT_COMMIT`, `GIT_BRANCH`, and `PIPELINE_VERSION` environment variables are baked into the image at build time and available at runtime for lineage tracking.

3. **Business Metrics**
   ```python
   log_metric("total_orders_processed", 1000000, date="2026-01-11")
   log_metric("revenue_processed_usd", 4567890.12, date="2026-01-11")
   log_metric("attribution_rate", 0.607, table="int_attributed_purchases")
   ```

4. **SLA Compliance**
   ```python
   {
     "table": "orders",
     "sla_definition": "silver_available_by_09:00",
     "completion_time": "2026-01-11T08:47:15Z",
     "sla_met": True,
     "margin_minutes": 13
   }
   ```

5. **Data Freshness**
   ```python
   {
     "table": "orders",
     "latest_partition_date": "2026-01-10",
     "hours_lag": 14.5,
     "sla_hours": 24,
     "status": "FRESH"
   }
   ```

---

## Query Examples: Getting Value from Metrics

### Detect Performance Regressions

```python
from src.observability import get_metrics_writer

writer = get_metrics_writer("pipeline_runs")
recent_runs = writer.read_metrics(limit=30)

# Calculate average duration
durations = [r["run_metadata"]["duration_seconds"] for r in recent_runs]
avg_duration = sum(durations) / len(durations)
print(f"Average pipeline duration: {avg_duration/60:.1f} minutes")

# Detect slowdowns
latest_duration = durations[0]
if latest_duration > avg_duration * 1.5:
    print(f"âš ï¸  Latest run 50% slower than average!")
```

### Track Data Quality Trends

```python
writer = get_metrics_writer("silver_quality")
quality_runs = writer.read_metrics(limit=10)

for run in quality_runs:
    for table in run["table_metrics"]:
        print(f"{table['table']}: {table['pass_rate']['rate']:.2%} pass rate")
```

### Generate SLA Report

```python
from datetime import datetime, timedelta

# Get last 30 days of runs
writer = get_metrics_writer("pipeline_runs")
runs = writer.read_metrics(limit=100)  # Assume ~3-4 runs/day

sla_threshold = 9 * 60 * 60  # 9 AM = 9 hours after midnight
sla_met_count = 0

for run in runs:
    completion_time = datetime.fromisoformat(run["run_metadata"]["end_time"])
    if completion_time.hour < 9:
        sla_met_count += 1

sla_compliance = sla_met_count / len(runs)
print(f"SLA compliance: {sla_compliance:.1%}")
```

---

## Summary: Answering Your Original Question

### Q: "Should this be a local path in dev and stored with data in production?"

**A: YES.** That's exactly the strategy implemented:

- **Local dev**: `data/metrics/` and `data/logs/` (git-ignored)
- **Production**: `gs://ecom-datalake-metrics/` (same environment as data)

### Q: "What other logging and tracking should we add?"

**A: We now track:**

1. âœ… **Pipeline execution** (duration, status, resource usage)
2. âœ… **Bronze validation** (metadata + Pydantic samples)
3. âœ… **Silver quality** (pass rates, quarantine analysis)
4. âœ… **Enriched transformations** (join rates, performance)
5. âœ… **Errors** (structured JSONL with full context)
6. âœ… **Audit trail** (all metric emissions)

**Additional recommendations:**
- Cost tracking (per table, per phase)
- Business metrics (revenue processed, order counts)
- SLA compliance tracking
- Data lineage (Git SHA, dbt versions)
- Alerting thresholds (integrate with Slack/PagerDuty)

### Q: "What does gold-standard production look like?"

**A: You now have it:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD STANDARD OBSERVABILITY                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  âœ… Environment-aware (local + prod)                   â”‚
â”‚  âœ… Structured metrics (JSON)                          â”‚
â”‚  âœ… Structured logs (JSONL)                            â”‚
â”‚  âœ… Comprehensive coverage (all pipeline phases)       â”‚
â”‚  âœ… SLA tracking (baselines + compliance)              â”‚
â”‚  âœ… Anomaly detection (vs historical trends)           â”‚
â”‚  âœ… Queryable (BigQuery integration ready)             â”‚
â”‚  âœ… Type-safe (full Python type hints)                 â”‚
â”‚  âœ… Zero-config (works out of box in local)            â”‚
â”‚  âœ… Production-ready (GCS integration built-in)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Files Created

- `src/observability/__init__.py` - Package exports
- `src/observability/config.py` - Environment-aware config
- `src/observability/metrics.py` - Metrics writers
- `src/observability/structured_logging.py` - JSONL logging
- `src/observability/audit.py` - Audit trail implementation
- `src/observability/README.md` - Usage documentation
- `docs/resources/OBSERVABILITY_STRATEGY.md` - This file

---

## Related Documentation

- **[CONFIG_STRATEGY.md](CONFIG_STRATEGY.md)** - Configuration hierarchy (observability env vars with overrides)
- **[ENVIRONMENT_VARIABLE_STRATEGY.md](ENVIRONMENT_VARIABLE_STRATEGY.md)** - All observability env vars documented
- **[CREDENTIALS_SAFETY_AUDIT.md](CREDENTIALS_SAFETY_AUDIT.md)** - GCP authentication for metrics/logs buckets
- **[DOCKER_VERSIONING.md](DOCKER_VERSIONING.md)** - Git metadata for lineage tracking
- **[SLA_AND_QUALITY.md](SLA_AND_QUALITY.md)** - Quality thresholds and monitoring expectations
- **[src/observability/README.md](../../src/observability/README.md)** - API usage guide

---

---

<p align="center">
  <a href="../../README.md">ğŸ  <b>Home</b></a>
  &nbsp;Â·&nbsp;
  <a href="../../RESOURCE_HUB.md">ğŸ“š <b>Resource Hub</b></a>
</p>

<p align="center">
  <sub>Last updated: 2026-01-24</sub><br>
  <sub>âœ¨ Transform the data. Tell the story. Build the future. âœ¨</sub>
</p>
